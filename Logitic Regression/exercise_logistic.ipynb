{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 1.  1.5 3.  2.  1. ]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1]).reshape(-1,1) \n",
    "print(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x12b572f10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:,0],y, c='red')\n",
    "plt.scatter(X[:,1],y, c=\"blue\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation du la regression logistique : \n",
    "\n",
    "1. Importer les librairies necessaires \n",
    "2. Assigner les données\n",
    "3. Regarder la forme des données. \n",
    "4. Visualiser et Comprendre les données. \n",
    "5. Voir si il y a des valeurs nules, et des doublons\n",
    "6. Gradient descent logistic (calculer d'un gradient)\n",
    "7. Gradient descent \n",
    "8. Regarder l'acuracy des prédictions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Importer les lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Assigner les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gradient Descent Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.49861806546328574\n",
      "dj_dw: [0.498333393278696, 0.49883942983996693]\n"
     ]
    }
   ],
   "source": [
    "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "w_tmp = np.array([2.,3.])\n",
    "b_tmp = 1.\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\" )\n",
    "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.684610468560574   \n",
      "Iteration 1000: Cost 0.1590977666870456   \n",
      "Iteration 2000: Cost 0.08460064176930081   \n",
      "Iteration 3000: Cost 0.05705327279402531   \n",
      "Iteration 4000: Cost 0.042907594216820076   \n",
      "Iteration 5000: Cost 0.034338477298845684   \n",
      "Iteration 6000: Cost 0.028603798022120097   \n",
      "Iteration 7000: Cost 0.024501569608793   \n",
      "Iteration 8000: Cost 0.02142370332569295   \n",
      "Iteration 9000: Cost 0.019030137124109114   \n",
      "\n",
      "updated parameters: w:[5.28123029 5.07815608], b:-14.222409982019837\n"
     ]
    }
   ],
   "source": [
    "w_tmp  = np.zeros_like(X_train[0])\n",
    "b_tmp  = 0.\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation de la Regression Logistique avec Scikit-learn\n",
    "\n",
    "Ungraded Lab: Logistic Regression using Scikit-Learn\n",
    "Goals\n",
    "In this lab you will:\n",
    "\n",
    "Train a logistic regression model using scikit-learn.\n",
    "Dataset\n",
    "Let's start with the same dataset as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model\n",
    "The code below imports the logistic regression model from scikit-learn. You can fit this model on the training data by calling fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions\n",
    "You can see the predictions made by this model by calling the predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X)\n",
    "\n",
    "print(\"Prediction on training set:\", y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy\n",
    "You can calculate this accuracy of this model by calling the score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set:\", lr_model.score(X, y))\n",
    "print(\"Accuracy on training set:\", lr_model.score(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLKaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
